{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3d3b833-19cc-4e66-b57d-4746a99b3a30",
   "metadata": {},
   "source": [
    "# GRAVITATIONAL LENS SDSSJ0819+5356"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5981a0d-5397-40f0-a831-b2a087b76042",
   "metadata": {},
   "source": [
    "### PSF and aperture photometry, GALSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c227781-54c0-4b39-804d-c54f07523717",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import utilfunctions as uf\n",
    "from astropy.io import fits\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib import ticker\n",
    "import numpy as np\n",
    "import warnings\n",
    "import cupy as cp\n",
    "# Packages used\n",
    "import astropy.wcs as wcs\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.stats import mad_std\n",
    "from astropy.wcs import WCS\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import copy\n",
    "import pyregion\n",
    "import scipy as sp\n",
    "from photutils.background import MedianBackground, MeanBackground, ModeEstimatorBackground, MMMBackground, SExtractorBackground, BiweightLocationBackground\n",
    "from photutils.background import Background2D\n",
    "from photutils.detection import DAOStarFinder\n",
    "from photutils.aperture import aperture_photometry, CircularAperture\n",
    "from astropy.nddata.utils import Cutout2D\n",
    "from photutils.psf import IntegratedGaussianPRF, SourceGrouper\n",
    "from photutils.background import MMMBackground, LocalBackground\n",
    "from photutils.psf import IterativePSFPhotometry, GriddedPSFModel\n",
    "from astropy.modeling.fitting import LevMarLSQFitter\n",
    "from astropy.table import QTable\n",
    "import datetime\n",
    "import math\n",
    "from itertools import combinations\n",
    "import PythonPhot as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1135033f-c4f5-48e7-9df0-a1117f6f81a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images_paths_QHY, images_paths_iKon, images_paths_all = uf.images_paths(\"SDSSJ0819+5356\", directory_path = \"work/red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7093839-32e2-4cf4-9969-f7b0d705f225",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Components coordinates\n",
    "coordsA=[124.998243440, 53.940423310]#A\n",
    "coordsB=[125.00007152, 53.93966377] #B ra_decB = w_image.pixel_to_world( xA+3.367/0.507, yA+2.226/0.507)\n",
    "coordsG=[124.999185780, 53.940057240] #G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6643d82e-4274-4ded-89ea-a62a2f40ea14",
   "metadata": {},
   "source": [
    "# _______________________________________________________________________\n",
    "### GALSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807e6561-317b-478d-9a03-49ee8861dbbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import galsim\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define galaxy parameters\n",
    "gal_x = 597.3053903003928   \n",
    "gal_y = 1064.9221439339417\n",
    "mag = 18.01\n",
    "half_light_radius = 5.700197238658777  # in pixels\n",
    "sersic_index = 6.2  # Adjusted to be within the valid range\n",
    "axis_ratio = 0.7500\n",
    "position_angle =  -35.5  # degrees\n",
    "\n",
    "# Calculate flux from magnitude (assuming a zero point of 25.0 for simplicity)\n",
    "zero_point = 24.5258  \n",
    "flux = 10**(-0.4 * (mag - zero_point))\n",
    "\n",
    "# Create the Sersic galaxy profile\n",
    "gal = galsim.Sersic(n=sersic_index, half_light_radius=half_light_radius, flux=flux)\n",
    "gal = gal.shear(q=axis_ratio, beta=position_angle * galsim.degrees)\n",
    "\n",
    "# Create an image with a sky background\n",
    "image_size = 2048  # Reduced image size to avoid memory issues\n",
    "pixel_scale = 0.507  # arcseconds per pixel\n",
    "\n",
    "# Define bounds to set the galaxy at the specific position within a smaller image\n",
    "bounds = galsim.BoundsI(1, image_size, 1, image_size)\n",
    "image = galsim.ImageF(bounds, scale=pixel_scale)\n",
    "offset_x = gal_x - image_size // 2\n",
    "offset_y = gal_y - image_size // 2\n",
    "offset = galsim.PositionD(offset_x, offset_y)\n",
    "\n",
    "# Draw the galaxy at the specified position\n",
    "gal.drawImage(image=image, method='real_space', offset=offset)\n",
    "\n",
    "# Add sky background\n",
    "sky_background = 472.987876  # ADUs\n",
    "image += sky_background\n",
    "\n",
    "# Save the image to a FITS file\n",
    "image.write(\"complex_galaxy.fits\")\n",
    "\n",
    "# Load and display the image\n",
    "image_data = fits.getdata(\"complex_galaxy.fits\")\n",
    "plt.imshow(image_data, origin='lower')#, cmap='gray', vmin=sky_background, vmax=sky_background + flux / 10)\n",
    "plt.colorbar()\n",
    "plt.xlim(gal_x-50, gal_x+50)\n",
    "plt.ylim(gal_y-50, gal_y+50)\n",
    "plt.title(\"Simulated Galaxy Image\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Galaxy image created and saved as 'complex_galaxy.fits'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4da9a5-81b8-466e-86b4-4ba8d6ff9088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import galsim\n",
    "import numpy as np\n",
    "\n",
    "# Define the parameters for the galaxy\n",
    "sersic_index = 6.2\n",
    "half_light_radius = 5.700197238658777\n",
    "axis_ratio = 0.75\n",
    "angle = -35.5 * galsim.degrees\n",
    "magnitude = 18.01\n",
    "zero_point = 24.5258\n",
    "pixel_scale = 0.507\n",
    "sky_level = 0#472.987876  # in ADU\n",
    "\n",
    "# Galaxy coordinates in a 2048x2048 image\n",
    "gal_x = 597.272008553425\n",
    "gal_y = 1064.9829494839112\n",
    "print(gal_x,gal_y)\n",
    "with fits.open(images_paths_iKon[100], memmap=True) as hdu:  \n",
    "    image, w_image, header, dateob, filter, fwhm, sky, EXPT1 = uf.getinfo(hdu)\n",
    "    print(len(image), images_paths_iKon[100] )\n",
    "    \n",
    "coords = [124.998243440, 53.940423310]\n",
    "x0, y0 = uf.radec_to_xy(coords,  w_image)\n",
    "gal_x=x0+(1.980/0.507)\n",
    "gal_y=y0+(1.348/0.507)\n",
    "print(gal_x,gal_y)\n",
    "\n",
    "\n",
    "# Create the Sersic galaxy profile\n",
    "galaxy = galsim.Sersic(n=sersic_index, half_light_radius=half_light_radius)\n",
    "galaxy = galaxy.shear(q=axis_ratio, beta=angle)\n",
    "\n",
    "# Adjust the galaxy flux to match the given magnitude\n",
    "galaxy = galaxy.withFlux(10**(-0.4 * (magnitude - zero_point)))\n",
    "\n",
    "# Load your custom PSF (assuming you have the PSF as an image file)\n",
    "psf_image = galsim.fits.read(\"PSFs/psfTTT1_iKon936-1_2023-11-19-02-58-58-157009_SDSSJ0819+5356.fits\")\n",
    "psf = galsim.InterpolatedImage(psf_image)\n",
    "\n",
    "# Convolve the galaxy with the PSF\n",
    "convolved_galaxy = galsim.Convolve([galaxy, psf])\n",
    "\n",
    "# Create a 2048x2048 image\n",
    "image = galsim.ImageF(2048, 2048, scale=pixel_scale)\n",
    "\n",
    "# Draw the convolved galaxy profile onto the image at the specified coordinates\n",
    "bounds = galsim.BoundsI(int(gal_x - 100), int(gal_x + 100), int(gal_y - 100), int(gal_y + 100))\n",
    "sub_image = image[bounds]\n",
    "convolved_galaxy.drawImage(image=sub_image, offset=galsim.PositionD(gal_x % 1 - 0.5, gal_y % 1 - 0.5))\n",
    "\n",
    "# Add sky background\n",
    "image += sky_level\n",
    "\n",
    "# Save the image to a file\n",
    "image.write('simulated_galaxy_with_custom_psf.fits')\n",
    "\n",
    "# Load and display the image\n",
    "image_data = fits.getdata('simulated_galaxy_with_custom_psf.fits')\n",
    "plt.imshow(image_data, origin='lower')#, cmap='gray', vmin=sky_background, vmax=sky_background + flux / 10)\n",
    "plt.colorbar()\n",
    "plt.xlim(gal_x-50, gal_x+50)\n",
    "plt.ylim(gal_y-50, gal_y+50)\n",
    "plt.title(\"Simulated Galaxy Image\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Simulation complete and image saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b6f87d-d06e-4090-8fab-974e1f0ce1f4",
   "metadata": {},
   "source": [
    "# _______________________________________________________________________\n",
    "### SERSIC model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed6e10f-bdd6-4852-9caa-655739d76a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from astropy.io import fits\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sersic_profile(x, y, x0, y0, n, Re, Ie, q, pa):\n",
    "    \"\"\"Generate a Sersic profile.\"\"\"\n",
    "    pa_rad = np.deg2rad(pa)\n",
    "    cos_pa = np.cos(pa_rad)\n",
    "    sin_pa = np.sin(pa_rad)\n",
    "    x_prime = (x - x0) * cos_pa + (y - y0) * sin_pa\n",
    "    y_prime = -(x - x0) * sin_pa + (y - y0) * cos_pa\n",
    "    r = np.sqrt((x_prime**2 + (y_prime/q)**2))\n",
    "    bn = 2 * n - 0.327\n",
    "    return Ie * np.exp(-bn * ((r / Re)**(1/n) - 1))\n",
    "\n",
    "# Define the parameters\n",
    "sersic_index = 6.2\n",
    "half_light_radius = 5.700197238658777\n",
    "axis_ratio = 0.75\n",
    "position_angle = -35.5\n",
    "magnitude = 18.01\n",
    "zero_point = 24.5258\n",
    "pixel_scale = 0.507\n",
    "sky_level = 472.987876  # in ADU\n",
    "image_size = 2048\n",
    "\n",
    "    \n",
    "# Galaxy coordinates in the image\n",
    "gal_x = 597.272008553425\n",
    "gal_y = 1064.9829494839112\n",
    "\n",
    "# Grid for the image\n",
    "y, x = np.mgrid[0:image_size, 0:image_size]\n",
    "\n",
    "# Calculate the effective intensity Ie\n",
    "Ie = 10**(-0.4 * (magnitude - zero_point)) / (2 * np.pi * half_light_radius**2 * axis_ratio * sersic_index)\n",
    "\n",
    "# Generate the Sersic profile\n",
    "galaxy_image = sersic_profile(x, y, gal_x, gal_y, sersic_index, half_light_radius, Ie, axis_ratio, position_angle)\n",
    "\n",
    "# Load and normalize the PSF image\n",
    "psf_image = fits.getdata(\"PSFs/psfTTT1_iKon936-1_2023-11-19-02-58-58-157009_SDSSJ0819+5356.fits\")\n",
    "psf_image = psf_image / np.sum(psf_image)\n",
    "\n",
    "# Convolve the galaxy profile with the PSF\n",
    "convolved_image = ndimage.convolve(galaxy_image, psf_image, mode='constant')\n",
    "\n",
    "# Scale the galaxy to be brighter than the sky\n",
    "galaxy_max = np.max(convolved_image)\n",
    "sky_max = sky_level\n",
    "convolved_image *= (sky_max / galaxy_max)\n",
    "\n",
    "# Add sky background\n",
    "final_image = convolved_image + sky_level\n",
    "\n",
    "# Save the final image to a FITS file\n",
    "hdu = fits.PrimaryHDU(final_image)\n",
    "hdu.writeto('simulated_galaxy_without_galsim.fits', overwrite=True)\n",
    "\n",
    "# Display the final image\n",
    "plt.imshow(final_image, origin='lower')\n",
    "plt.colorbar()\n",
    "plt.xlim(gal_x-50, gal_x+50)\n",
    "plt.ylim(gal_y-50, gal_y+50)\n",
    "plt.title('Simulated Galaxy with Custom PSF')\n",
    "plt.show()\n",
    "\n",
    "print(\"Simulation complete and image saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883b1f29-d77e-4b8c-892a-0bb2fdd3c503",
   "metadata": {},
   "outputs": [],
   "source": [
    "with fits.open(images_paths_iKon[100], memmap=True) as hdu:  \n",
    "    image, w_image, header, dateob, filter, fwhm, sky, EXPT1 = uf.getinfo(hdu)\n",
    "    print(len(image), images_paths_iKon[100] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05000352-a4c0-471a-9ce4-bcc31bec1077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load and display the image\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "im0 = axs[0].imshow(image.get(), vmin=0, vmax=1050, cmap='viridis')\n",
    "axs[0].set_title('Original Image')\n",
    "axs[0].set_xlabel('X [pixels]')\n",
    "axs[0].set_ylabel('Y [pixels]')\n",
    "#axs[0].plot(gal_x, gal_y)\n",
    "# axs[0].plot(597.3053903003928-554, 1064.9221439339417-1019, \"x\")\n",
    "cbar0 = fig.colorbar(im0, ax=axs[0])\n",
    "axs[0].set_xlim(635, 554)\n",
    "axs[0].set_ylim(1100, 1019)\n",
    "cbar0.set_label('Intensity')\n",
    "\n",
    "# Model\n",
    "im1 = axs[1].imshow(image_data[1019:1100,554:635], origin='lower', cmap='viridis')\n",
    "axs[1].set_title('MODEL')\n",
    "axs[1].set_xlabel('X [pixels]')\n",
    "axs[1].set_ylabel('Y [pixels]')\n",
    "# axs[1].plot(593.174812838104-554, 1062.6750714184982-1019, \"x\")\n",
    "cbar1 = fig.colorbar(im1, ax=axs[1])\n",
    "cbar1.set_label('Intensity')\n",
    "\n",
    "# Model\n",
    "im2 = axs[2].imshow((image.get()-image_data),vmin=0, vmax=1050,  cmap='viridis')\n",
    "axs[2].set_title('REST')\n",
    "axs[2].set_xlabel('X [pixels]')\n",
    "axs[2].set_ylabel('Y [pixels]')\n",
    "axs[2].set_xlim(635, 554)\n",
    "axs[2].set_ylim(1100, 1019)\n",
    "# axs[2].plot(593.174812838104-554, 1062.6750714184982-1019, \"x\") -2*image_data)[1019:1100,554:635]\n",
    "cbar2 = fig.colorbar(im2, ax=axs[2])\n",
    "cbar2.set_label('Intensity')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778667ad-4b41-4371-a82a-7ce7b8708ac4",
   "metadata": {},
   "source": [
    "# _______________________________________________________________________\n",
    "### APERTURE Photometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7944676-1fd9-49bb-aff5-5aa254b189a4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from astropy.visualization import simple_norm\n",
    "import matplotlib.pyplot as plt\n",
    "from photutils.detection import DAOStarFinder\n",
    "from photutils.aperture import CircularAperture\n",
    "from photutils.aperture import aperture_photometry\n",
    "coords = cp.asarray([[125.037078920, 53.968357790], [125.013046340, 53.934497430], [125.012248390, 53.931427250], [124.998243440, 53.940423310], [124.998243440, 53.940423310],[125.010332700, 53.927527680]]) \n",
    "#coords = cp.asarray([[124.998243440, 53.940423310],[124.998243440, 53.940423310],[125.037078920, 53.968357790], [125.013046340, 53.934497430], [125.012248390, 53.931427250], [125.010332700, 53.927527680], [124.985519970, 53.943565380], [125.024931110, 53.921033100], [124.985143180, 53.950553540], [125.023014140, 53.950554040]]) # A, B, D, E, F, G, H, I, J, K\n",
    "coordsstars = cp.asarray([[125.037078920, 53.968357790], [125.013046340, 53.934497430], [125.010332700, 53.927527680], [125.024931110, 53.921033100], [125.023014140, 53.950554040]]) # D, E, G, I, K\n",
    "\n",
    "estrella1=[]\n",
    "estrella2=[]\n",
    "estrella3=[]\n",
    "estrella4=[]\n",
    "A=[]\n",
    "B=[]\n",
    "time=[]\n",
    "filtro=[]\n",
    "fwhms=[]\n",
    "count=0\n",
    "for i in images_paths_iKon:\n",
    "    with fits.open(i, memmap=True) as hdu:\n",
    "        try: \n",
    "            image, w_image, header, dateob, filter, fwhm, sky, EXPT1 = uf.getinfo(hdu)\n",
    "            x0, y0 = uf.radec_to_xy(coordsstars[0].get(), w_image)\n",
    "            x1, y1 = uf.radec_to_xy(coordsstars[1].get(), w_image)\n",
    "            x2, y2 = uf.radec_to_xy(coordsstars[2].get(), w_image)\n",
    "            x3, y3 = uf.radec_to_xy(coordsstars[3].get(), w_image)\n",
    "            x4, y4 = uf.radec_to_xy(coordsstars[4].get(), w_image)\n",
    "            try:\n",
    "                \n",
    "                sources = np.array([[x0,x1,x2,x3,x4],[y0,y1,y2,y3,y4]])\n",
    "                positions = np.transpose((sources[0], sources[1]))\n",
    "                \n",
    "                apertures = CircularAperture(positions, r=0.5)\n",
    "                \n",
    "                phot_table = aperture_photometry(image.get(), apertures)\n",
    "                if count==0:\n",
    "                    print(phot_table)\n",
    "                    count+=1\n",
    "                estrella1.append(phot_table[\"aperture_sum\"][0])\n",
    "                estrella2.append(phot_table[\"aperture_sum\"][1])\n",
    "                estrella3.append(phot_table[\"aperture_sum\"][2])\n",
    "                estrella4.append(phot_table[\"aperture_sum\"][3])\n",
    "                \n",
    "                #A.append(phot_table[\"aperture_sum\"][3])\n",
    "                B.append(phot_table[\"aperture_sum\"][4])\n",
    "                time.append(dateob)\n",
    "                filtro.append(filter)\n",
    "                fwhms.append(fwhm)\n",
    "            \n",
    "            except:\n",
    "                continue\n",
    "        except:\n",
    "                continue\n",
    "#print(len(estrella1),len(estrella2),len(A),len(B),len(time), len(filtro))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d6c620-c80d-4672-94ff-007857782503",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "diferent_filters = np.unique(filtro)\n",
    "print(diferent_filters)\n",
    "goodseeing = np.where(np.array(fwhms) < (50/0.508))[0]\n",
    "\n",
    "estrella12=[np.array([]) for i in range(len(diferent_filters))]\n",
    "estrella22=[np.array([]) for i in range(len(diferent_filters))]\n",
    "estrella32=[np.array([]) for i in range(len(diferent_filters))]\n",
    "estrella42=[np.array([]) for i in range(len(diferent_filters))]\n",
    "A2=[np.array([]) for i in range(len(diferent_filters))]\n",
    "B2=[np.array([]) for i in range(len(diferent_filters))]\n",
    "time2 = [np.array([]) for i in range(len(diferent_filters))]\n",
    "for i in range(len(diferent_filters)):\n",
    "    index = np.where(np.array(filtro) == diferent_filters[i])[0]\n",
    "    for j in index:\n",
    "        if j in goodseeing:   \n",
    "            estrella12[i] = np.append(estrella12[i], estrella1[j])\n",
    "            estrella22[i] = np.append(estrella22[i], estrella2[j])\n",
    "            estrella32[i] = np.append(estrella32[i], estrella3[j])\n",
    "            estrella42[i] = np.append(estrella42[i], estrella4[j])\n",
    "            #A2[i] = np.append(A2[i], A[j])\n",
    "            B2[i] = np.append(B2[i], B[j])\n",
    "            time2[i] = np.append(time2[i], time[j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a9ec13-0a3a-4275-8572-169caa538efa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a=[50,50,50,50,50,50,50]\n",
    "figure, axis = plt.subplots(1, 6,figsize=(50, 10))\n",
    "figure.tight_layout(pad=3.5)\n",
    "dif=[np.asarray([]) for i in range(len(estrella12))]\n",
    "for i in range(len(estrella12)):\n",
    "    for j, f in enumerate(estrella12[i]):\n",
    "        dif[i] = np.append(dif[i], -2.5*np.log10(f/estrella42[i][j]))\n",
    "    difmag = -2.5*np.log10(np.array(estrella12[i])/np.array(estrella42[i]))\n",
    "    difmag2 = -2.5*np.log10(np.array(estrella22[i])/np.array(estrella42[i]))\n",
    "    difmag3 = -2.5*np.log10(np.array(estrella32[i])/(np.array(estrella12[i])+np.array(estrella22[i])+np.array(estrella42[i])))\n",
    "    difmag4 = -2.5*np.log10(np.array(B2[i])/(np.array(estrella12[i])+np.array(estrella22[i])+np.array(estrella32[i])+np.array(estrella42[i])))\n",
    "#plt.errorbar(time2[0],difmag,yerr=np.std((difmag[np.logical_not(np.isnan(difmag))])), elinewidth=0.75, linewidth=0,   marker=\".\",label = f\"σ = {np.std((difmag[np.logical_not(np.isnan(difmag))]))}\")\n",
    "#plt.errorbar(time2[0],difmag2,yerr=np.std((difmag2[np.logical_not(np.isnan(difmag2))])), elinewidth=0.75, linewidth=0,   marker=\".\",label = f\"σ = {np.std((difmag2[np.logical_not(np.isnan(difmag2))]))}\")\n",
    "    axis[i].errorbar(time2[i], dif[i], yerr=0, elinewidth=0.75, linewidth=0,   marker=\".\", label = f\"σ = {np.std((difmag[np.logical_not(np.isnan(difmag3))]))}\")\n",
    "    #axis[i].errorbar(time2[i], difmag2, yerr=0, elinewidth=0.75, linewidth=0,   marker=\".\",label = f\"σ = {np.std((difmag2[np.logical_not(np.isnan(difmag4))]))}\")\n",
    "    #axis[i].errorbar(time2[i], difmag3, yerr=0, elinewidth=0.75, linewidth=0,   marker=\".\",label = f\"σ = {np.std((difmag3[np.logical_not(np.isnan(difmag4))]))}\")\n",
    "    #axis[i].errorbar(time2[i], difmag4, yerr=0, elinewidth=0.75, linewidth=0,   marker=\".\",label = f\"σ = {np.std((difmag4[np.logical_not(np.isnan(difmag4))]))}\")\n",
    "    axis[i].legend()\n",
    "#plt.savefig('galfit'+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c9ebd0-3b27-43b4-b463-709ad31791a3",
   "metadata": {},
   "source": [
    "# _______________________________________________________________________\n",
    "### PSF Photometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc668510-af2a-4c48-956b-56e211bf0c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSF PHOTOMETRY USING DAOPHOT\n",
    "\n",
    "warnings.filterwarnings(\"ignore\") # Not showing warmings\n",
    "\n",
    "image_paths = images_paths_iKon\n",
    "\n",
    "coords = cp.asarray([[124.998243440, 53.940423310],[124.998243440, 53.940423310],[125.037078920, 53.968357790], [125.013046340, 53.934497430], [125.010332700, 53.927527680], [125.024931110, 53.921033100], [125.023014140, 53.950554040]]) # D, E, G, I, K\n",
    "#coords = cp.asarray([[124.998243440, 53.940423310],[124.998243440, 53.940423310],[125.037078920, 53.968357790], [125.013046340, 53.934497430], [125.012248390, 53.931427250], [125.010332700, 53.927527680], [124.985519970, 53.943565380], [125.024931110, 53.921033100], [124.985143180, 53.950553540], [125.023014140, 53.950554040]]) # A, B, D, E, F, G, H, I, J, K\n",
    "coordsstars = cp.asarray([[124.998243440, 53.940423310],[124.998243440, 53.940423310],[125.037078920, 53.968357790], [125.013046340, 53.934497430], [125.010332700, 53.927527680], [125.024931110, 53.921033100], [125.023014140, 53.950554040]]) # D, E, G, I, K\n",
    "\n",
    "# Components coordinates\n",
    "coordsA=[124.998243440, 53.940423310]#A\n",
    "coordsB=[125.00007152, 53.93966377] #B ra_decB = w_image.pixel_to_world( xA+3.367/0.507, yA+2.226/0.507)\n",
    "coordsG=[124.999185780, 53.940057240] #G\n",
    "\n",
    "\n",
    "bkg_estimator = LocalBackground(5, 10 , MMMBackground())\n",
    "psf_model = IntegratedGaussianPRF()\n",
    "daogroup = SourceGrouper(2.0)  # Group stars that are closer than 2 pixels\n",
    "fitter = LevMarLSQFitter()\n",
    "\n",
    "images_with_a_KeyErrorstars = np.array([])\n",
    "images_other_errorstars = np.array([])\n",
    "filters = np.array([])\n",
    "dateobs = np.array([])\n",
    "fwhms = np.array([])\n",
    "flux = [np.asarray([]) for i in range(len(coordsstars))]  # A, B, D, E, F, G, H, I, J, K\n",
    "eflux = [np.asarray([]) for i in range(len(coordsstars))]  # A, B, D, E, F, G, H, I, J, K\n",
    "EXPTs = np.array([])\n",
    "count=0\n",
    "for i in image_paths:\n",
    "    \n",
    "    with fits.open(i, memmap=True) as hdu:\n",
    "        try:\n",
    "            \n",
    "            image, w_image, header, dateob, filter, fwhm, sky, EXPT1 = uf.getinfo(hdu)\n",
    "            daofind = DAOStarFinder(fwhm=fwhm, threshold=50) \n",
    "            photometry = IterativePSFPhotometry(\n",
    "            finder=daofind,\n",
    "            grouper=daogroup,\n",
    "            psf_model=psf_model,\n",
    "            localbkg_estimator=bkg_estimator,\n",
    "            fitter=fitter,\n",
    "            maxiters=1,\n",
    "            fit_shape=(11, 11),\n",
    "            aperture_radius=1.0)\n",
    "            \n",
    "\n",
    "            if count==100:\n",
    "                daofind = DAOStarFinder(fwhm=fwhm, threshold=100)\n",
    "                print(sky)\n",
    "                sources = daofind(image.get())\n",
    "                print(\"hola\")\n",
    "                \n",
    "            for j in range(len(coordsstars)):\n",
    "                \n",
    "                if j==1:\n",
    "                    x1, y1 = uf.radec_to_xy(coordsstars[j].get(), w_image)\n",
    "                    init_params = QTable()\n",
    "                    init_params['x'] = [x1+(3.367/0.507)]\n",
    "                    init_params['y'] = [y1+(2.226/0.507)]\n",
    "                else:\n",
    "                    x1, y1 = uf.radec_to_xy(coordsstars[j].get(), w_image)\n",
    "                    init_params = QTable()\n",
    "                    init_params['x'] = [x1]\n",
    "                    init_params['y'] = [y1]\n",
    "                \n",
    "                #print(photometry(image.get(), init_params=init_params))\n",
    "                result_tab = photometry(image.get(), init_params=init_params)\n",
    "                #print(result_tab)\n",
    "                result_tab = result_tab.to_pandas()\n",
    "                \n",
    "                flux[j] = np.append(flux[j], result_tab[\"flux_fit\"][0])\n",
    "                eflux[j] = np.append(eflux[j], result_tab[\"flux_err\"][0])\n",
    "                #if i == images_paths_iKon[100] and j==3:\n",
    "                    #plt.imshow(image.get(), vmin=200, vmax=1000)\n",
    "                    #plt.plot( x1, y1, \"x\", mec='r', ms=50)\n",
    "                    #plt.plot( x1-3.367/0.508, y1-2.226/0.508, \"x\", mec='r', ms=50)\n",
    "                    #plt.plot( result_tab[\"x_fit\"], result_tab[\"y_fit\"], \"x\", mec='b', ms=50)\n",
    "                    #plt.plot(x1-(1.980/0.508),y1-(1.348/0.508), \"x\", mec='g', ms=50)\n",
    "                    #plt.xlim( x1+20, x1-20)\n",
    "                    #plt.ylim( y1+20, y1-20)\n",
    "                    #print(result_tab)\n",
    "            filters = np.append(filters, filter)\n",
    "            dateobs = np.append(dateobs, dateob)\n",
    "            fwhms = np.append(fwhms, fwhm)\n",
    "            EXPTs = np.append(EXPTs, EXPT1)\n",
    "            \n",
    "        except KeyError:\n",
    "            images_with_a_KeyErrorstars = np.append(images_with_a_KeyErrorstars, i)\n",
    "\n",
    "        except:\n",
    "            images_other_errorstars = np.append(images_other_errorstars, i)\n",
    "            \n",
    "    count+=1\n",
    "    \n",
    "print(len(images_paths_iKon), len(images_with_a_KeyErrorstars), len(images_other_errorstars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbac545d-3417-44d6-9f16-f1c8cdedf926",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function for differential photometry between stars with the flux and the dateobs in arrays\n",
    "def star_diff_phot(fluxstars, dateobs, diferent_filters, starsname = False): \n",
    "    # fluxstars: array separated by filters, in each filter the flux values of each star in different arrays, for a star each value of flux corresponds to a different image.\n",
    "    # dateobs: array separated by filters, each date corresponds to a different image.\n",
    "    # starsname: array with names of the stars, default the stars are named with numbers.\n",
    "    # diferent_filters: array with the filters\n",
    "    \n",
    "    if starsname == False: # Stars named with numbers\n",
    "        starsname = [str(i) for i in range(len(fluxstars[0]))]\n",
    "    \n",
    "    diff_phot = {} # Diff. phot values\n",
    "    diff_phot_names = {} # Diff. phot star combinations names\n",
    "    diff_phot_dates = {} # Dates for each diff. phot value\n",
    "    \n",
    "    for num_stars in range(1, len(starsname)): # Differential photometry using different numbers of stars like reference stars\n",
    "        diff_phot[num_stars] = {} \n",
    "        diff_phot_names[num_stars] = {}\n",
    "        diff_phot_dates[num_stars] = {}\n",
    "                           \n",
    "        for i, flux_star in enumerate(fluxstars):  # Separated for each filter\n",
    "            diff_phot[num_stars]['filter_' + diferent_filters[i]] = []\n",
    "            diff_phot_names[num_stars]['filter_' + diferent_filters[i]] = []\n",
    "            diff_phot_dates[num_stars]['filter_' + diferent_filters[i]] = []  \n",
    "            \n",
    "            for j, star_flux in enumerate(flux_star): # Differential photometry of one star in front of the rest\n",
    "                for combo in combinations(np.delete(range(len(flux_star)), j), num_stars): # Combinations of the stars used like reference stars\n",
    "                    diffphot = -2.5 * np.log10(sum(flux_star[k] for k in combo) / star_flux ) #  sum(flux_star[k] for k in combo) / star_flux\n",
    "                    diff_phot[num_stars]['filter_' + diferent_filters[i]].append(diffphot)\n",
    "                    diff_phot_names[num_stars]['filter_' + diferent_filters[i]].append(starsname[j] + ''.join(starsname[k] for k in combo))\n",
    "                    diff_phot_dates[num_stars]['filter_' + diferent_filters[i]].append(dateobs[i])\n",
    "\n",
    "    return diff_phot, diff_phot_names, diff_phot_dates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90b0e9b-d45a-4a33-84bf-be1129d534c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def error_diff_phot(fluxstars, errorfluxstars, diferent_filters, starsname = False):\n",
    "    if starsname == False: # Stars named with numbers\n",
    "        starsname = [str(i) for i in range(len(fluxstars[0]))]\n",
    "    \n",
    "    error = {}\n",
    "    errordiff_phot_names = {}\n",
    "    for num_stars in range(1, len(starsname)): # Differential photometry using different numbers of stars like reference stars\n",
    "        error[num_stars] = {} \n",
    "        errordiff_phot_names[num_stars] = {}                  \n",
    "        for i, flux_star in enumerate(fluxstars):  # Separated for each filter\n",
    "            error[num_stars]['filter_' + diferent_filters[i]] = []\n",
    "            errordiff_phot_names[num_stars]['filter_' + diferent_filters[i]] = []\n",
    "            for j, star_flux in enumerate(flux_star): # Differential photometry of one star in front of the rest\n",
    "                for combo in combinations(np.delete(range(len(flux_star)), j), num_stars): # Combinations of the stars used like reference stars\n",
    "                    errordiffphot = 2.5 * np.log10(np.e) * np.sqrt(sum((errorfluxstars[i][k]**2) for k in combo)/((sum(flux_star[k] for k in combo))**2) + (errorfluxstars[i][j]/star_flux)**2) \n",
    "                    error[num_stars]['filter_' + diferent_filters[i]].append(errordiffphot)\n",
    "                    errordiff_phot_names[num_stars]['filter_' + diferent_filters[i]].append(starsname[j] + ''.join(starsname[k] for k in combo))\n",
    "\n",
    "    return error, errordiff_phot_names                \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857a3992-ded4-40f5-8d17-88b94f894d64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function for differential photometry between the components of the lens and the stars with the flux and the dateobs in arrays\n",
    "def component_diff_phot(fluxstars, fluxcomp, dateobs, diferent_filters, starsname = False, compname = False): \n",
    "    # fluxstars: array separated by filters, in each filter the flux values of each star in different arrays, for a star each value of flux corresponds to a different image.\n",
    "    # fluxcomp: array separated by filters, in each filter the flux values of each component in different arrays, for a component each value of flux corresponds to a different image.\n",
    "    # dateobs: array separated by filters, each date corresponds to a different image.\n",
    "    # starsname: array with names of the stars, default the stars are named with numbers.\n",
    "    # compname: array with names of the components, default the components are named with numbers.\n",
    "    # diferent_filters: array with the filters\n",
    "    \n",
    "    if starsname == False: # Stars named with numbers\n",
    "        starsname = [str(i) for i in range(len(fluxstars[0]))]\n",
    "        \n",
    "    if compname == False: # Components named with numbers\n",
    "        compname = [str(i) for i in range(len(fluxcomp[0]))]\n",
    "    \n",
    "    #diff_phot = {} # Diff. phot values\n",
    "    #diff_phot_names = {} # Diff. phot components and stars combinations names\n",
    "    #diff_phot_dates = {} # Dates for each diff. phot value\n",
    "\n",
    "    diff_photA = {} # Diff. phot values\n",
    "    diff_phot_namesA = {} # Diff. phot components and stars combinations names\n",
    "    diff_phot_datesA = {} # Dates for each diff. phot value\n",
    "    \n",
    "    diff_photB = {} # Diff. phot values\n",
    "    diff_phot_namesB = {} # Diff. phot components and stars combinations names\n",
    "    diff_phot_datesB = {} # Dates for each diff. phot value\n",
    "    \n",
    "    for num_stars in range(1, len(starsname)+1): # Differential photometry using different numbers of stars like reference stars\n",
    "        #diff_phot[num_stars] = {} \n",
    "        #diff_phot_names[num_stars] = {}\n",
    "        #diff_phot_dates[num_stars] = {}\n",
    "\n",
    "        diff_photA[num_stars] = {} \n",
    "        diff_phot_namesA[num_stars] = {}\n",
    "        diff_phot_datesA[num_stars] = {}\n",
    "        \n",
    "        diff_photB[num_stars] = {} \n",
    "        diff_phot_namesB[num_stars] = {}\n",
    "        diff_phot_datesB[num_stars] = {}\n",
    "        \n",
    "                           \n",
    "        for i, flux_star in enumerate(fluxstars):  # Separated for each filter\n",
    "            #diff_phot[num_stars]['filter_' + diferent_filters[i]] = []\n",
    "            #diff_phot_names[num_stars]['filter_' + diferent_filters[i]] = []\n",
    "            #diff_phot_dates[num_stars]['filter_' + diferent_filters[i]] = [] \n",
    "\n",
    "            diff_photA[num_stars]['filter_' + diferent_filters[i]] = []\n",
    "            diff_phot_namesA[num_stars]['filter_' + diferent_filters[i]] = []\n",
    "            diff_phot_datesA[num_stars]['filter_' + diferent_filters[i]] = []  \n",
    "            \n",
    "            diff_photB[num_stars]['filter_' + diferent_filters[i]] = []\n",
    "            diff_phot_namesB[num_stars]['filter_' + diferent_filters[i]] = []\n",
    "            diff_phot_datesB[num_stars]['filter_' + diferent_filters[i]] = []\n",
    "            \n",
    "            for j, comp_flux in enumerate(fluxcomp[i]): # Differential photometry of one component in front of the stars\n",
    "                for combo in combinations(range(len(flux_star)), num_stars): # Combinations of the stars used like reference stars\n",
    "                    if j==0:\n",
    "                        diffphotA = -2.5 * np.log10(comp_flux / sum(flux_star[k] for k in combo))\n",
    "                        diff_photA[num_stars]['filter_' + diferent_filters[i]].append(diffphotA)\n",
    "                        diff_phot_namesA[num_stars]['filter_' + diferent_filters[i]].append(compname[j] + ''.join(starsname[k] for k in combo))\n",
    "                        diff_phot_datesA[num_stars]['filter_' + diferent_filters[i]].append(dateobs[i])\n",
    "                        \n",
    "                    else:\n",
    "                        \n",
    "                        diffphotB = -2.5 * np.log10(comp_flux / sum(flux_star[k] for k in combo))  \n",
    "                        diff_photB[num_stars]['filter_' + diferent_filters[i]].append(diffphotB)\n",
    "                        diff_phot_namesB[num_stars]['filter_' + diferent_filters[i]].append(compname[j] + ''.join(starsname[k] for k in combo))\n",
    "                        diff_phot_datesB[num_stars]['filter_' + diferent_filters[i]].append(dateobs[i])\n",
    "                        \n",
    "                    #diffphot = -2.5 * np.log10(comp_flux / sum(flux_star[k] for k in combo)) \n",
    "                    #diff_phot[num_stars]['filter_' + diferent_filters[i]].append(diffphot)\n",
    "                    #diff_phot_names[num_stars]['filter_' + diferent_filters[i]].append(compname[j] + ''.join(starsname[k] for k in combo))\n",
    "                    #diff_phot_dates[num_stars]['filter_' + diferent_filters[i]].append(dateobs[i])\n",
    "\n",
    "                    \n",
    "\n",
    "    return diff_photA, diff_phot_namesA, diff_phot_datesA, diff_photB, diff_phot_namesB, diff_phot_datesB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8e8608-f109-46bb-803d-cd9a85d5e592",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function for compute the scale factors for the σ of differential photometry between components and stars\n",
    "def scalefactors(fluxstars, errorfluxstars, fluxcomp, errorfluxcomp, diferent_filters, starsname = False):\n",
    "    # fluxstars: array separated by filters, in each filter the flux values of each star in different arrays, for a star each value of flux corresponds to a different image.\n",
    "    # fluxcomp: array separated by filters, in each filter the flux values of each component in different arrays, for a component each value of flux corresponds to a different image.\n",
    "    # diferent_filters: array with the filters\n",
    "    # starsname: array with names of the stars, default the stars are named with numbers.\n",
    "    # compname: array with names of the components, default the components are named with numbers.\n",
    "    # diferent_filters: array with the filters\n",
    "    \n",
    "    if starsname == False: # Stars named with numbers\n",
    "        starsname = [str(i) for i in range(len(fluxstars[0]))]\n",
    "    \n",
    "    #scale_factors = {} # Scale factors (SF) -> σ_comp = SF * σ_stars_after_plot\n",
    "    #scale_factors_names = {} # To know which components and which stars have been used \n",
    "\n",
    "    scale_factorsA = {} # Scale factors (SF) -> σ_comp = SF * σ_stars_after_plot\n",
    "    scale_factorsB = {} \n",
    "    scale_factors_namesA = {} # To know which components and which stars have been used \n",
    "    scale_factors_namesB = {}\n",
    "    \n",
    "    for num_stars in range(1, len(starsname)): # Number of reference stars\n",
    "        #scale_factors[num_stars] = {}\n",
    "        #scale_factors_names[num_stars] = {}\n",
    "\n",
    "        scale_factorsA[num_stars] = {}\n",
    "        scale_factors_namesA[num_stars] = {}\n",
    "        scale_factorsB[num_stars] = {}\n",
    "        scale_factors_namesB[num_stars] = {}\n",
    "        \n",
    "        for i, flux_star in enumerate(fluxstars): # Separated for each filter\n",
    "            #scale_factors[num_stars]['filter_' + diferent_filters[i]] = []\n",
    "            #scale_factors_names[num_stars]['filter_' + diferent_filters[i]] = []\n",
    "\n",
    "            scale_factorsA[num_stars]['filter_' + diferent_filters[i]] = []\n",
    "            scale_factors_namesA[num_stars]['filter_' + diferent_filters[i]] = []\n",
    "            scale_factorsB[num_stars]['filter_' + diferent_filters[i]] = []\n",
    "            scale_factors_namesB[num_stars]['filter_' + diferent_filters[i]] = []\n",
    "            \n",
    "            for j, comp_flux in enumerate(fluxcomp[i]):\n",
    "                for combo in combinations(range(len(flux_star)), num_stars): # Implementation the sum of stars' fluxes\n",
    "                    for s in range(len(flux_star)): \n",
    "                        if s not in combo: # Stars no used in the combo to compute the σ_stars_from_flux_error needed for SF = σ_stars_from_flux_error/σ_comp_from_flux_error\n",
    "                            #scale_factors[num_stars]['filter_' + diferent_filters[i]].append((flux_star[s] / comp_flux) ** 2 * (\n",
    "                                        #((sum(flux_star[k] for k in combo))**2 * (errorfluxcomp[i][j])**2 +\n",
    "                                         #(comp_flux)**2 * (sum(errorfluxstars[i][k] for k in combo))**2) /\n",
    "                                        #(flux_star[s]** 2 * (sum(errorfluxstars[i][k] for k in combo))**2 +\n",
    "                                         #errorfluxstars[i][s]** 2 * (sum(flux_star[k] for k in combo))**2)))\n",
    "                            #if j == 0: # A\n",
    "                                #scale_factors_names[num_stars]['filter_' + diferent_filters[i]].append(\"A\" + starsname[s] + ''.join(starsname[k] for k in combo))\n",
    "                            #else: # B\n",
    "                                #scale_factors_names[num_stars]['filter_' + diferent_filters[i]].append(\"B\" + starsname[s] + ''.join(starsname[k] for k in combo))   \n",
    "                            if j == 0: # A\n",
    "                                scale_factorsA[num_stars]['filter_' + diferent_filters[i]].append((flux_star[s] / comp_flux) ** 2 * (\n",
    "                                        ((sum(flux_star[k] for k in combo))**2 * (errorfluxcomp[i][j])**2 +\n",
    "                                         (comp_flux)**2 * (sum(errorfluxstars[i][k] for k in combo))**2) /\n",
    "                                        (flux_star[s]** 2 * (sum(errorfluxstars[i][k] for k in combo))**2 +\n",
    "                                         errorfluxstars[i][s]** 2 * (sum(flux_star[k] for k in combo))**2)))\n",
    "                                scale_factors_namesA[num_stars]['filter_' + diferent_filters[i]].append(\"A\" + starsname[s] + ''.join(starsname[k] for k in combo))\n",
    "                            \n",
    "                            else: # B  \n",
    "                                scale_factorsB[num_stars]['filter_' + diferent_filters[i]].append((flux_star[s] / comp_flux) ** 2 * (\n",
    "                                        ((sum(flux_star[k] for k in combo))**2 * (errorfluxcomp[i][j])**2 +\n",
    "                                         (comp_flux)**2 * (sum(errorfluxstars[i][k] for k in combo))**2) /\n",
    "                                        (flux_star[s]** 2 * (sum(errorfluxstars[i][k] for k in combo))**2 +\n",
    "                                         errorfluxstars[i][s]** 2 * (sum(flux_star[k] for k in combo))**2)))\n",
    "                                scale_factors_namesB[num_stars]['filter_' + diferent_filters[i]].append(\"B\" + starsname[s] + ''.join(starsname[k] for k in combo))\n",
    "    return scale_factorsA, scale_factors_namesA, scale_factorsB, scale_factors_namesB\n",
    "    #return scale_factors, scale_factors_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf62674-b54a-44e3-8c94-f66ba5ee5ced",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "diferent_filters = np.unique(filters)\n",
    "goodseeing = np.where(fwhms < (50/0.508))[0]\n",
    "\n",
    "fwhms2 = [np.array([]) for i in range(len(diferent_filters))]\n",
    "dateobs2 = [np.array([]) for i in range(len(diferent_filters))]\n",
    "EXPTs2 = [np.array([]) for i in range(len(diferent_filters))]\n",
    "\n",
    "\n",
    "indexfilters = [np.array([]) for i in range(len(diferent_filters))]\n",
    "\n",
    "for i in range(len(diferent_filters)):\n",
    "    index = np.where(filters == diferent_filters[i])[0]\n",
    "    \n",
    "    for j in index:\n",
    "        \n",
    "        if j in goodseeing:       \n",
    "        \n",
    "            fwhms2[i] = np.append(fwhms2[i], fwhms[j])\n",
    "            dateobs2[i] = np.append(dateobs2[i], dateobs[j])\n",
    "            EXPTs2[i] = np.append(EXPTs2[i], EXPTs[j])\n",
    "            \n",
    "            indexfilters[i] = np.append(indexfilters[i], j)\n",
    "\n",
    "#print(indexfilters)\n",
    "# Group the fluxes in stars' fluxes and components' fluxes and group them in diferent filter. Also the errors.\n",
    "errorfluxstars0=[]\n",
    "fluxstars0=[]\n",
    "fluxcomp0=[]\n",
    "errorfluxcomp0=[]\n",
    "\n",
    "for i in range(len(diferent_filters)):\n",
    "    errorinter = [np.array([]) for i in range(len(flux)-2)]\n",
    "    inter = [np.array([]) for i in range(len(flux)-2)]\n",
    "    intercomp = [np.array([]) for i in range(2)]\n",
    "    errorintercomp = [np.array([]) for i in range(2)]\n",
    "    # totes les estrelles per cada filtre\n",
    "    for j in range(len(flux)):\n",
    "        if j<2:\n",
    "            for k in indexfilters[i]:\n",
    "                intercomp[j] = np.append(intercomp[j], flux[j][int(k)])\n",
    "                errorintercomp[j] = np.append(errorintercomp[j], eflux[j][int(k)])\n",
    "            #for k in indexfilters[i]:\n",
    "                #errorinter[j] = np.append(errorinter[j], eflux[j][int(k)])\n",
    "                #inter[j] = np.append(inter[j], flux[j][int(k)])\n",
    "                \n",
    "        else:\n",
    "            for k in indexfilters[i]:\n",
    "                errorinter[j-2] = np.append(errorinter[j-2], eflux[j][int(k)])\n",
    "                inter[j-2] = np.append(inter[j-2], flux[j][int(k)])\n",
    "            #for k in indexfilters[i]:\n",
    "                #intercomp[j-2] = np.append(intercomp[j-2], flux[j][int(k)])\n",
    "                #errorintercomp[j-2] = np.append(errorintercomp[j-2], eflux[j][int(k)])\n",
    "                \n",
    "    errorfluxstars0.append(errorinter)\n",
    "    fluxstars0.append(inter) #afegim filtres amb les estrelles separades\n",
    "    fluxcomp0.append(intercomp)\n",
    "    errorfluxcomp0.append(errorintercomp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846fe08f-4b5b-435c-b4fb-e956ade9d9c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "fluxstars = copy.deepcopy(fluxstars0)\n",
    "errorfluxstars = copy.deepcopy(errorfluxstars0)\n",
    "fluxcomp = copy.deepcopy(fluxcomp0)\n",
    "errorfluxcomp = copy.deepcopy(errorfluxcomp0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e41e1db-c6ce-4fc4-8dcb-35d09e1c741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the fluxes from the psf photometry, differential photometry and scale factor calculations\n",
    "compname = [\"A\",\"B\"]\n",
    "starsname = [\"D\", \"E\", \"G\", \"I\", \"K\"]\n",
    "diff_phot_compA, diff_phot_names_compA, diff_phot_dates_compA, diff_phot_compB, diff_phot_names_compB, diff_phot_dates_compB = component_diff_phot(fluxstars, fluxcomp, dateobs2, diferent_filters, starsname, compname)\n",
    "diff_phot, diff_phot_names, diff_phot_dates = star_diff_phot(fluxstars, dateobs2, diferent_filters, starsname)\n",
    "scale_factorsA, scale_factors_namesA, scale_factorsB, scale_factors_namesB = scalefactors(fluxstars, errorfluxstars, fluxcomp, errorfluxcomp, diferent_filters, starsname)\n",
    "error, errordiff_phot_names = error_diff_phot(fluxstars, errorfluxstars, diferent_filters, starsname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4973c5-a152-450b-8c12-e95a4cbb9686",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a=[250,250,250,250,250,250,250,250,250,250,250]\n",
    "for o, t in enumerate(diff_phot):\n",
    "    figure, axis = plt.subplots(len(diff_phot[t]), len(diff_phot[t][\"filter_Lum\"]),figsize=(a[o], 60))\n",
    "    figure.tight_layout(pad=3.5)\n",
    "    for i, filtre in enumerate(diff_phot[t]):\n",
    "        for j, phot in enumerate(diff_phot[t][filtre]):\n",
    "        \n",
    "            axis[i][j].errorbar(diff_phot_dates[t]['filter_' + diferent_filters[i]][j]-2460000.0, phot, yerr=np.std((phot[np.logical_not(np.isnan(phot))])), elinewidth=0.75, linewidth=0,   marker=\".\", label = f\"σ = {np.std((phot[np.logical_not(np.isnan(phot))]))}\")# + r' $\\bar{σ}$ = ' + f\"{np.mean(error[t]['filter_' + diferent_filters[i]][j][np.logical_not(np.isnan(error[t]['filter_' + diferent_filters[i]][j]))])}\")\n",
    "            axis[i][j].legend()\n",
    "            #axis[i][j].set_ylim([np.mean(group3_2[i][j][np.logical_not(np.isnan(group3_2[i][j]))])-10*np.std(group3_2[i][j][np.logical_not(np.isnan(group3_2[i][j]))]),np.mean(group3_2[i][j][np.logical_not(np.isnan(group3_2[i][j]))])+10*np.std(group3_2[i][j][np.logical_not(np.isnan(group3_2[i][j]))])])\n",
    "            axis[i][j].set(xlabel='Time JD-2460000', ylabel='Phot. Diff.')\n",
    "            axis[i][j].set_title(\"Filter \"+diferent_filters[i]+ \" star \" + diff_phot_names[t]['filter_' + diferent_filters[i]][j][0]+ \" and ref. star \" + diff_phot_names[t]['filter_' + diferent_filters[i]][j][1:] )\n",
    "    # plt.show()\n",
    "    # plt.savefig('diff_phot_stars'+str(t)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6827b62c-ccd7-4fd8-a4ae-b4d03dbfac95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a=[500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500]\n",
    "for o, t in enumerate(scale_factors_names):\n",
    "    #print(o)\n",
    "    #if o<2:\n",
    "        figure, axis = plt.subplots(len(scale_factors_names[t]), len(scale_factors_names[t][\"filter_Lum\"]),figsize=(a[o], 60))\n",
    "        figure.tight_layout(pad=3.5)\n",
    "\n",
    "        for i, filtre in enumerate(scale_factors_names[t]):\n",
    "            for j, l in enumerate(scale_factors_names[t][filtre]):\n",
    "                for r, n in enumerate(diff_phot_names[t]['filter_' + diferent_filters[i]]):\n",
    "                    if n==l[1:]:\n",
    "                        for g, phot in enumerate(diff_phot_comp[t]['filter_' + diferent_filters[i]]):\n",
    "                            #print(diff_phot_names_comp[t]['filter_' + diferent_filters[i]][g], n, l, phot )\n",
    "                            if l[0]==diff_phot_names_comp[t]['filter_' + diferent_filters[i]][g][0] and diff_phot_names_comp[t]['filter_' + diferent_filters[i]][g][1:]==n[1:]: \n",
    "\n",
    "                                axis[i][j].errorbar(diff_phot_dates_comp[t]['filter_' + diferent_filters[i]][g]-2460000.0, phot, yerr=scale_factors[t][filtre][j]*np.std((diff_phot[t][filtre][r][np.logical_not(np.isnan(diff_phot[t][filtre][r]))])), elinewidth=0.75, linewidth=0,   marker=\".\", label = f\"σ = $\\Gamma ^2 $ x {np.std((diff_phot[t][filtre][r][np.logical_not(np.isnan(diff_phot[t][filtre][r]))]))}\")\n",
    "                                axis[i][j].legend()\n",
    "                                axis[i][j].set(xlabel='Time JD-2460000', ylabel='Phot. Diff.')\n",
    "                                axis[i][j].set_title(\"Filter \"+diferent_filters[i] + \" comp. \"+ diff_phot_names_comp[t]['filter_' + diferent_filters[i]][g][0] + \" and ref. star \" + diff_phot_names_comp[t]['filter_' + diferent_filters[i]][g][1:] +\", $\\Gamma ^2 $ using\"+ l +\", σ_stars_\"+ n )\n",
    "        #print(\"good\")\n",
    "        #break\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cde3f9-234c-4588-8d4b-2d8a9877d13c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "diff_phot_dates2 = copy.deepcopy(diff_phot_dates)\n",
    "error2 = copy.deepcopy(error)\n",
    "for o, t in enumerate(diff_phot):\n",
    "    std2=[]\n",
    "    stdmeanpercentile=[]\n",
    "    stdpercentile2=[]\n",
    "    combos2sorted=[]\n",
    "    for i, filters in enumerate(diff_phot[t]):\n",
    "        if i!=0:\n",
    "            stdinter=[]\n",
    "            stdpercentileinter=[]\n",
    "            stdmeaninter=[]\n",
    "            #count=0\n",
    "            for j, combo in enumerate(diff_phot[t][filters]):\n",
    "                deletepositions2=[]\n",
    "                deletepositions3=[]\n",
    "                # for o, l in enumerate(np.logical_not(np.isnan(combo))):\n",
    "                    # if l == np.bool_(False):\n",
    "                        # diff_phot_dates2[t][filters][j] = np.delete(diff_phot_dates2[t][filters][j], o)\n",
    "                        # error2[t]['filter_' + diferent_filters[i]][j] = np.delete(error2[t]['filter_' + diferent_filters[i]][j], o)\n",
    "                combo=(combo)[np.logical_not(np.isnan(combo))]    \n",
    "                err = error2[t]['filter_' + diferent_filters[i]][j]\n",
    "                for k in combo:\n",
    "                    if k < np.quantile(combo, 0.1) or k > np.quantile(combo, 0.9):     \n",
    "                        deletepositions2.append(np.where(combo == k)[0][0])  \n",
    "                for k in err:\n",
    "                    if k < np.quantile(err, 0.3) or k > np.quantile(err, 0.7):     \n",
    "                        deletepositions3.append(np.where(err == k)[0][0]) \n",
    "                combo = np.delete(combo, deletepositions2)\n",
    "                \n",
    "                err = np.delete(err, deletepositions2+deletepositions3)\n",
    "                #err = np.delete(err, deletepositions3)\n",
    "                err= err[np.logical_not(np.isnan(err))]\n",
    "                stdinter.append(np.std(combo))\n",
    "                stdpercentileinter.append(np.std(combo))\n",
    "                \n",
    "                stdmeaninter.append(np.mean(err))\n",
    "                \n",
    "            sortered = sorted(zip(stdpercentileinter,stdinter,diff_phot_names[t][filters],stdmeaninter))\n",
    "            std2.append([x[1] for x in sortered])\n",
    "            stdpercentile2.append([x[0] for x in sortered])\n",
    "            combos2sorted.append([x[2] for x in sortered])\n",
    "            stdmeanpercentile.append([x[3] for x in sortered])\n",
    "\n",
    "    figure, axis = plt.subplots(1, len(stdpercentile2),figsize=(60, 10), layout='constrained')\n",
    "    #figure.tight_layout(pad=3.5)\n",
    "    for i in range(len(stdpercentile2)):\n",
    "        x = np.arange(len(combos2sorted[i]))\n",
    "        width = 0.3\n",
    "        multiplier = 0\n",
    "        #bars=axis[i].bar(combos3sorted[i], stdpercentile[i])\n",
    "        roundvalues=[round(val, 4) for val in stdpercentile2[i]]\n",
    "        roundvalues2=[round(val, 4) for val in stdmeanpercentile[i]]\n",
    "        a=[stdpercentile2[i]]#, stdmeanpercentile[i]]\n",
    "        b=[roundvalues]#, roundvalues2]\n",
    "        #print(roundvalues2)\n",
    "        for p in range(0,2):\n",
    "            offset = width * multiplier\n",
    "            axis[i].bar_label(axis[i].bar(x + offset, a[p], width), labels=  b[p], padding=3)\n",
    "            multiplier += 1\n",
    "        axis[i].set(xlabel='Star groups', ylabel='Standard deviation')\n",
    "        axis[i].set_title(\"Filter \"+ diferent_filters[i+1])\n",
    "        axis[i].set_xticks(x, combos2sorted[i])\n",
    "    \n",
    "    #plt.savefig('standarddeviation+propagationerrorPSF'+str(t)+'.png')\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1165f5-bbde-49d4-97bc-1e4b94513f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a=[250,400,250,100]\n",
    "for o, t in enumerate(scale_factors_names):\n",
    "    figure, axis = plt.subplots(len(scale_factors_names[t]), len(scale_factors_names[t][\"filter_Lum\"]),figsize=(a[o], 60))\n",
    "    figure.tight_layout(pad=3.5)\n",
    "    for i, filtre in enumerate(scale_factors_names[t]):\n",
    "        for j, l in enumerate(scale_factors_names[t][filtre]):\n",
    "            for r, n in enumerate(diff_phot_names[t]['filter_' + diferent_filters[i]]):\n",
    "                if n==l[1:]:\n",
    "                    for g, phot in enumerate(diff_phot_comp[t]['filter_' + diferent_filters[i]]):\n",
    "                        #print(diff_phot_names_comp[t]['filter_' + diferent_filters[i]][g], n, l, phot )\n",
    "                        if l[0]==diff_phot_names_comp[t]['filter_' + diferent_filters[i]][g][0] and diff_phot_names_comp[t]['filter_' + diferent_filters[i]][g][1:]==n[1:]: #scale_factors[t][filtre][j]*\n",
    "                            \n",
    "                            axis[i][j].errorbar(diff_phot_dates_comp[t]['filter_' + diferent_filters[i]][g]-2460000.0, phot, yerr=np.std((diff_phot[t][filtre][r][np.logical_not(np.isnan(diff_phot[t][filtre][r]))])), elinewidth=0.75, linewidth=0,   marker=\".\", label = f\"σ = $\\Gamma ^2 $ x {np.std((diff_phot[t][filtre][r][np.logical_not(np.isnan(diff_phot[t][filtre][r]))]))}\")\n",
    "                            axis[i][j].legend()\n",
    "                            axis[i][j].set(xlabel='Time JD-2460000', ylabel='Phot. Diff.')\n",
    "                            axis[i][j].set_title(\"Filter \"+diferent_filters[i] + \" comp. \"+ diff_phot_names_comp[t]['filter_' + diferent_filters[i]][g][0] + \" and ref. star \" + diff_phot_names_comp[t]['filter_' + diferent_filters[i]][g][1:] +\", $\\Gamma ^2 $ using\"+ l +\", σ_stars_\"+ n )\n",
    "   #plt.show()\n",
    "    \n",
    "    #plt.savefig('diff_phot_components+error'+str(t)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031864b3-26fb-43d9-bf36-88fa2bf5e418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract outliers\n",
    "def exctract_outliers(data, min_quantile, max_quantile):\n",
    "    \n",
    "    deletepositions=[]\n",
    "    \n",
    "    for k in data:\n",
    "        if k < np.quantile(data, min_quantile) or k > np.quantile(data, max_quantile):                                    \n",
    "            deletepositions.append(np.where(data == k)[0][0])\n",
    "            \n",
    "    return deletepositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3f9350-8b99-4827-8315-f66ba3333fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exctract_outliers2(data, time, min_quantile, max_quantile):\n",
    "    deletepositions=[]\n",
    "    \n",
    "    df = pd.DataFrame({'X': time, 'Y': data})\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(time.reshape(-1, 1), data, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Fit the model\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    # Predict on the training set\n",
    "    df['Predicted'] = model.predict(time.reshape(-1, 1))\n",
    "    \n",
    "    # Calculate residuals\n",
    "    df['Residuals'] = df['Y'] - df['Predicted']\n",
    "\n",
    "    # Standardize residuals\n",
    "    df['StdResiduals'] = (df['Residuals'] - df['Residuals'].mean()) / df['Residuals'].std()\n",
    "\n",
    "    # Identify outliers using Z-score method (threshold |Z| > 2 for this example)\n",
    "    outliers = df[np.abs(df['StdResiduals']) > 2]\n",
    "    \n",
    "    for k in outliers[\"Y\"]:\n",
    "        deletepositions.append(np.where(data == k)[0][0])\n",
    "    # print(len(df['Predicted']), len(data), len(time))\n",
    "    return deletepositions, df['Predicted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bc01a0-1880-4a0e-a5f1-fadbc12bfe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __data_time_binning(data, time, mjd_ini, mjd_end, time_interval_hours, parameter, parameter_error):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: panda dataframe. Mandatory to have columns [time,parameter,parameter_error] i.e. ['MJD','MAG','eMAG']\n",
    "    time: column time name \n",
    "    mjd_ini/mjd_end: MJD initial/final float\n",
    "    time_interval_hours: time interval units hours float\n",
    "    parameter: column parameter name to do stats string\n",
    "    parameter_error: column parameter name with individual errors string\n",
    "    \n",
    "    \n",
    "    Return\n",
    "    ----------\n",
    "    result_df: panda df [time+'_min', time+'_max', time+'_median',time+'std',time+'_n', parameter+'_median', parameter+'_std', parameter+'_n',time+'_central','e'+time]\n",
    "    \"\"\"    \n",
    "    MJD_bins=int((mjd_end-mjd_ini)*24/time_interval_hours)\n",
    "    \n",
    "    x_median,y_median,n_x,n_y = __binXY(data[time],data[parameter],statistic='median',xbins=MJD_bins,xrange=None)\n",
    "    # print(x_median,y_median,n_x,n_y)\n",
    "    x_std,y_std,n_x,n_y = __binXY(data[time],data[parameter],statistic='std',xbins=MJD_bins,xrange=None)\n",
    "    x_min,y_min,n_x,n_y = __binXY(data[time],data[parameter],statistic='min',xbins=MJD_bins,xrange=None)\n",
    "    x_max,y_max,n_x,n_y = __binXY(data[time],data[parameter],statistic='max',xbins=MJD_bins,xrange=None)\n",
    "    data_bin=pd.DataFrame({time+'_min': x_min, time+'_max': x_max, time+'_median': x_median, time+'_std': x_std, time+'_n': n_x, parameter+'_median': y_median, parameter+'_std': y_std, parameter+'_n': n_y})\n",
    "    data_bin = data_bin.dropna() # dropping NaN lines\n",
    "    data_bin['e'+parameter] = data_bin[parameter+'_std'] / np.sqrt(data_bin[parameter+'_n'])\n",
    "    data_bin[time+'_central']=data_bin[time+'_min']+(data_bin[time+'_max']-data_bin[time+'_min'])/2\n",
    "    data_bin['e'+time]=(data_bin[time+'_max']-data_bin[time+'_min'])/2\n",
    "    return(data_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff967df-1be7-441c-a0ec-4b5986104945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __binXY(x,y,statistic='mean',xbins=10,xrange=None):\n",
    "    \"\"\"\n",
    "    Finds statistical value of x and y values in each x bin. \n",
    "    Returns the same type of statistic for both x and y.\n",
    "    See scipy.stats.binned_statistic() for options.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array\n",
    "        x values.\n",
    "    y : array\n",
    "        y values.\n",
    "    statistic : string or callable, optional\n",
    "        See documentation for scipy.stats.binned_statistic(). Default is mean.\n",
    "    xbins : int or sequence of scalars, optional\n",
    "        If xbins is an integer, it is the number of equal bins within xrange.\n",
    "        If xbins is an array, then it is the location of xbin edges, similar\n",
    "        to definitions used by np.histogram. Default is 10 bins.\n",
    "        All but the last (righthand-most) bin is half-open. In other words, if \n",
    "        bins is [1, 2, 3, 4], then the first bin is [1, 2) (including 1, but \n",
    "        excluding 2) and the second [2, 3). The last bin, however, is [3, 4], \n",
    "        which includes 4.    \n",
    "        \n",
    "    xrange : (float, float) or [(float, float)], optional\n",
    "        The lower and upper range of the bins. If not provided, range is \n",
    "        simply (x.min(), x.max()). Values outside the range are ignored.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    x_stat : array\n",
    "        The x statistic (e.g. mean) in each bin. \n",
    "    y_stat : array\n",
    "        The y statistic (e.g. mean) in each bin.       \n",
    "    n_x : array of dtype int\n",
    "        The count of x values in each bin.\n",
    "    n_y : array of dtype int\n",
    "        The count of y values in each bin.        \n",
    "        \"\"\"\n",
    "    x_stat, xbin_edges, binnumber = stats.binned_statistic(x, x, \n",
    "                                 statistic=statistic, bins=xbins, range=xrange)\n",
    "    #print(x_stat)\n",
    "    y_stat, xbin_edges, binnumber = stats.binned_statistic(x, y, \n",
    "                                 statistic=statistic, bins=xbins, range=xrange)\n",
    "    n_x, xbin_edges, binnumber = stats.binned_statistic(x, x, \n",
    "                                 statistic='count', bins=xbins, range=xrange)\n",
    "    n_y, xbin_edges, binnumber = stats.binned_statistic(x, y, \n",
    "                                 statistic='count', bins=xbins, range=xrange)\n",
    "            \n",
    "    return x_stat, y_stat, n_x, n_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31563dfb-55dc-499a-8cd5-3a657f1052e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTS COMPONENTS + STARS + SCALE FACTORS, substract outliers and binning (improve it)\n",
    "import copy\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import statistics as stat\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "a=[250,400,250,100]\n",
    "for o, t in enumerate(scale_factors_namesA):\n",
    "    #print(len(scale_factors_namesA), len(scale_factors_namesB))\n",
    "    \n",
    "    figure, axis = plt.subplots(len(scale_factors_namesA[t]), int(len(scale_factors_namesA[t][\"filter_Lum\"])),figsize=(a[o], 60))\n",
    "    figure.tight_layout(pad=3.5)\n",
    "    for i, filtre in enumerate(scale_factors_namesA[t]):\n",
    "        #print(len(scale_factors_namesA[t]), len(scale_factors_namesB[t]))\n",
    "        for j, l in enumerate(scale_factors_namesA[t][filtre]):\n",
    "            #print(len(scale_factors_namesA[t][filtre]), len(scale_factors_namesB[t][filtre]))\n",
    "            for r, n in enumerate(diff_phot_names[t]['filter_' + diferent_filters[i]]):\n",
    "                #print(len(scale_factors_namesB[t][filtre][j]), len(scale_factors_namesB[t][filtre][j]))\n",
    "                if n==l[1:]:\n",
    "                    for g, phot in enumerate(diff_phot_compA[t]['filter_' + diferent_filters[i]]):\n",
    "                        \n",
    "                        if l[0]==diff_phot_names_compA[t]['filter_' + diferent_filters[i]][g][0] and  diff_phot_names_compA[t]['filter_' + diferent_filters[i]][g][1:]==n[1:]: \n",
    "                            \n",
    "                            photA = copy.deepcopy(phot)\n",
    "                            datesA = copy.deepcopy(diff_phot_dates_compA[t]['filter_' + diferent_filters[i]][g])\n",
    "                            sfA = copy.deepcopy(scale_factorsA[t][filtre][j])\n",
    "                            \n",
    "                            deletepositions0 = exctract_outliers(data = photA,  min_quantile = 0.03, max_quantile = 0.97)\n",
    "                            photA = np.delete(photA, deletepositions0)\n",
    "                            datesA = np.delete(datesA, deletepositions0)\n",
    "                            sfA = np.delete(sfA, deletepositions0)\n",
    "                            \n",
    "                            #deletepositions, pred1 = exctract_outliers2(data = photA, time = datesA,  min_quantile = 0.03, max_quantile = 0.97)\n",
    "                            # print(len(deletepositions), len(pred1), len(datesA), len(photA))\n",
    "                            #photA = np.delete(photA, deletepositions)\n",
    "                            #datesA = np.delete(datesA, deletepositions)\n",
    "                            #sfA = np.delete(sfA, deletepositions)\n",
    "                            #pred1 = np.delete(pred1, deletepositions)\n",
    "                        \n",
    "                            \n",
    "                            photB = copy.deepcopy(diff_phot_compB[t]['filter_' + diferent_filters[i]][g])\n",
    "                            datesB = copy.deepcopy(diff_phot_dates_compB[t]['filter_' + diferent_filters[i]][g])\n",
    "                            sfB = copy.deepcopy(scale_factorsB[t][filtre][j])\n",
    "                              \n",
    "                            deletepositions02 = exctract_outliers(data = diff_phot_compB[t]['filter_' + diferent_filters[i]][g],min_quantile = 0.03, max_quantile = 0.97)\n",
    "                            photB = np.delete(photB, deletepositions02)\n",
    "                            datesB = np.delete(datesB, deletepositions02)     \n",
    "                            sfB = np.delete(sfB, deletepositions02)\n",
    "                            \n",
    "                            #deletepositions2, pred2 = exctract_outliers2(data = photB, time = datesB, min_quantile = 0.03, max_quantile = 0.97)\n",
    "                            # print(len(pred2), len(datesB), len(photB))                     \n",
    "                            #photB = np.delete(photB, deletepositions2)\n",
    "                            #datesB = np.delete(datesB, deletepositions2)     \n",
    "                            #sfB = np.delete(sfB, deletepositions2)  \n",
    "                            #pred2 = np.delete(pred2, deletepositions2)\n",
    "                            # print(len(pred2), len(datesB), len(photB))\n",
    "                            \n",
    "                            #deletepositions3=[]\n",
    "                            #for u, x in enumerate(sfA):\n",
    "                            #    if x > (stat.mode(sfA)+stat.mode(sfA)/2):# or x < (stat.mode(sfA)-stat.mode(sfA)/2):\n",
    "                            #        deletepositions3.append(u)\n",
    "                                                                       \n",
    "                            #photA = np.delete(photA, deletepositions3)\n",
    "                            #datesA = np.delete(datesA, deletepositions3)\n",
    "                            #sfA = np.delete(sfA, deletepositions3)\n",
    "                            #pred1 = np.delete(pred1, deletepositions3)\n",
    "\n",
    "                            \n",
    "                            #deletepositions4=[]\n",
    "                            #for u, x in enumerate(sfB):\n",
    "                           #     if x > (stat.mode(sfB)+stat.mode(sfB)/2):# or x < (stat.mode(sfB)-stat.mode(sfB)):\n",
    "                           #         deletepositions4.append(u)\n",
    "                            #photB = np.delete(photB, deletepositions4)\n",
    "                            #datesB = np.delete(datesB, deletepositions4)\n",
    "                            #sfB = np.delete(sfB, deletepositions4)\n",
    "                           # pred2 = np.delete(pred2, deletepositions4)\n",
    "\n",
    "                        \n",
    "                            err=diff_phot[t][filtre][r][np.logical_not(np.isnan(diff_phot[t][filtre][r]))]\n",
    "                            #deletepositions5=[]\n",
    "                            #for k in diff_phot[t][filtre][r][np.logical_not(np.isnan(diff_phot[t][filtre][r]))]:\n",
    "                                #if k < np.quantile(err, 0.1) or k > np.quantile(err, 0.9):     \n",
    "                                    #deletepositions5.append(np.where(err == k)[0][0]) \n",
    "                            \n",
    "                            deletepositions5 = exctract_outliers(data = diff_phot[t][filtre][r][np.logical_not(np.isnan(diff_phot[t][filtre][r]))], min_quantile = 0.1, max_quantile = 0.9)\n",
    "                            \n",
    "                            err = np.delete(err, deletepositions5)\n",
    "                            #(datesA, photA, sfA*np.std(err))\n",
    "                            if len(sfA*np.std(err))>1 and  len(sfB*np.std(err))>1:\n",
    "                                lista3= zip(datesA, photA, sfA*np.std(err))\n",
    "                                # print(type(lista3))\n",
    "                                lista4= zip(datesB, photB, sfB*np.std(err))\n",
    "                                dfA = pd.DataFrame(lista3, columns=['MJD','MAG','eMAG'])\n",
    "                                dfB = pd.DataFrame(lista4, columns=['MJD','MAG','eMAG'])\n",
    "                                    #print(dfA)\n",
    "\n",
    "                                data_binA = __data_time_binning(data=dfA, time='MJD', mjd_ini=min(datesA), mjd_end=max(datesA), time_interval_hours=24.0, parameter='MAG', parameter_error='eMAG')\n",
    "                                data_binB = __data_time_binning(data=dfB, time='MJD', mjd_ini=min(datesB), mjd_end=max(datesB), time_interval_hours=24.0, parameter='MAG', parameter_error='eMAG')\n",
    "\n",
    "                                # datesA2, photA2, ephotA2 = bin_flux_data_julian(photA, datesA, 24)\n",
    "                                # datesB2, photB2, ephotB2 = bin_flux_data_julian(photB, datesB, 24)\n",
    "                                # datesA3, sfA2, ee = bin_flux_data_julian(sfA, datesA, 24)\n",
    "                                # datesB3, sfB2, ee = bin_flux_data_julian(sfB, datesB, 24)\n",
    "\n",
    "                                if i == 1:\n",
    "                                    if j==0:\n",
    "                                        finalA=data_binA[\"MAG_median\"]\n",
    "                                        finalB=data_binB[\"MAG_median\"]\n",
    "                                        finaldataA=data_binA[\"MJD_central\"]\n",
    "                                        finaldataB=data_binB[\"MJD_central\"]\n",
    "\n",
    "                                # for q,w in enumerate(ephotA2):\n",
    "                                #     if w==0:\n",
    "                                #         ephotA2[q]=sfA2[q]*np.std((err))\n",
    "                                # for q,w in enumerate(ephotB2):\n",
    "                                #     if w==0:\n",
    "                                #         ephotB2[q]=sfB2[q]*np.std((err))  pred1\n",
    "                                # print(photA)\n",
    "                                axis[i][j].errorbar((datesA)-2460000.0, photA, yerr=sfA*np.std((err)), elinewidth=0.75, linewidth=0,   marker=\".\", label = f\"Component A ($σA _i = \\Gamma ^{2}_i $ x {np.std((err))})\") # ({np.mean(sfA)})*np.std((diff_phot[t][filtre][r][np.logical_not(np.isnan(diff_phot[t][filtre][r]))]))\n",
    "                                axis[i][j].errorbar((datesB)-2460000.0, photB, yerr=sfB*np.std((err)), elinewidth=0.75, linewidth=0,   marker=\".\", label = f\"Component B ($σB _i = \\Gamma ^{2}_i $ x {np.std((err))})\") #({np.mean(sfB)})*np.std((diff_phot[t][filtre][r][np.logical_not(np.isnan(diff_phot[t][filtre][r]))]))\n",
    "\n",
    "                                # axis[i][j].errorbar((data_binA[\"MJD_central\"])-2460000.0, data_binA[\"MAG_median\"], xerr=data_binA[\"eMJD\"], yerr=data_binA[\"eMAG\"], elinewidth=0.75, linewidth=0, ms=10,  marker=\".\", mfc=\"black\",mec=\"black\", ecolor=\"black\")#, label = f\"$σA _i = \\Gamma ^{2}_i $ x {np.std((diff_phot[t][filtre][r][np.logical_not(np.isnan(diff_phot[t][filtre][r]))]))}\") # ({np.mean(sfA)})*np.std((diff_phot[t][filtre][r][np.logical_not(np.isnan(diff_phot[t][filtre][r]))]))\n",
    "                                # axis[i][j].errorbar((data_binB[\"MJD_central\"])-2460000.0, data_binB[\"MAG_median\"], xerr=data_binB[\"eMJD\"], yerr=data_binB[\"eMAG\"], elinewidth=0.75, linewidth=0, ms=10,  marker=\".\", mfc=\"black\",mec=\"black\", ecolor=\"black\")\n",
    "\n",
    "                                axis[i][j].locator_params(axis='x', nbins=50)\n",
    "\n",
    "                                axis[i][j].legend()\n",
    "                                axis[i][j].set(xlabel='Time JD-2460000', ylabel='Phot. Diff.')\n",
    "                                axis[i][j].set_title(\"Filter \"+diferent_filters[i] + \" comp. \"+ diff_phot_names_compA[t]['filter_' + diferent_filters[i]][g][0]+\" and \" + diff_phot_names_compB[t]['filter_' + diferent_filters[i]][g][0]+ \" and ref. star \" + diff_phot_names_compA[t]['filter_' + diferent_filters[i]][g][1:] +\", $\\Gamma ^2 $ using \"+ l+ \" and \"+ scale_factors_namesB[t][filtre][j] +\", σ_stars_\"+ n ) #\" and \" + diff_phot_names_compB[t]['filter_' + diferent_filters[i]][g][0]\n",
    "    \n",
    "    #plt.savefig('yes'+'.jpeg')\n",
    "    \n",
    "    # plt.savefig('diff_phot_components_error_binning'+str(t)+'.jpeg')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
